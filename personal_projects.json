[
  {
    "name": "Visual Sudoku puzzle solver",
    "description": "<p>\n    This is a Proof-of-Concept attempt to detect and extract a Sudoku Puzzle from an image by using standard Computer\n    Vision methods. No Supervised Machine Learning methods are used and that's intentional. Goal was to experiment with\n    OpenCV and the standard CV techniques for de-noising and processing an image, recognize patterns and provide \n    visualization.\n</p>\n\n<div style=\"text-align: center;\"><img src=\"res%2Ffull%20solution.gif\"  width=\"600\" height=\"400\" alt=\"full solution\" /></div>\n\n<p>There are 5 stages:</p>\n\n<ul>\n    <li><strong>Clear the image from the noise</strong> so only the sudoku puzzle is left in the image</li>\n    <li><strong>Find the pattern of the grid</strong> and extract the sudoku with the digits</li>\n    <li><strong>Locate and identify the digits</strong></li>\n    <li><strong>Find the solution of the sudoku</strong> (out of scope for this project)</li>\n    <li><strong>Present the solution back</strong> to the original image</li>\n</ul>\n\n",
    "link": "https://github.com/jimkon/sudoku-solver",
    "tags": ["All", "Computer Vision", "Python"],
    "show": true
  },
  {
    "name": "Mecon App",
    "description": "App for visualising and tagging my transaction data",
    "link": "https://github.com/jimkon/mecon",
    "tags": ["All", "Flask", "App", "Python"]
  },
  {
    "name": "Python source metrics",
    "description": "App for visualising and tagging my transaction data",
    "link": "https://github.com/jimkon/mecon",
    "tags": ["All", "Flask", "App", "Python"]
  },
  {
    "name": "Alpha star python package",
    "description": "no description yet",
    "link": "https://github.com/jimkon/adaptive-discretization",
    "tags": ["All", "Algorithms", "Artificial Intelligence", "Python"]
  },
  {
    "name": "Adaptive Discretisation package",
    "description": "no description yet",
    "link": "https://github.com/jimkon/adaptive-discretization",
    "tags": ["All", "Algorithms", "Python"]
  },
  {
    "name": "Implementation of Deep Reinforcement Learning in Large Discrete Action Spaces",
    "description": "<p>Link to <a href=\"https://arxiv.org/abs/1512.07679\">paper</a></p>\n\n<p>\n    Implementation of the algorithm in Python 3, TensorFlow, and OpenAI Gym.\n</p>\n\n<p>\n    This paper introduces the Wolpertinger training algorithm that extends the Deep Deterministic Policy Gradient training\n    algorithm introduced in <a href=\"https://arxiv.org/abs/1509.02971\">this</a> paper.\n</p>\n\n<p>\n    I used and extended <strong>stevenpjg</strong>'s implementation of <strong>DDPG</strong> algorithm found\n    <a href=\"https://github.com/stevenpjg/ddpg-aigym\">here</a> licensed under the MIT license.\n</p>\n\n<p>\n    Master is currently <strong>only for continuous action spaces</strong>.\n</p>\n\n<p>\n    The branch discrete-and-continuous provides the ability to use the discrete environments of the gym.\n</p>\n",
    "link": "https://github.com/jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces",
    "tags": ["All", "Reinforcement Learning", "Neural Networks", "Artificial Intelligence", "Python"],
    "show": true

  },
    {
    "name": "Implementation of Adaptation of Action Space for Reinforcement Learning",
    "description": "<h2>Summary</h2>\n\n<p>\n    Reinforcement Learning is a Machine Learning technique, where a decision-making algorithm, also known as an autonomous agent, interacts with an (unknown) environment by making observations and taking actions, while it is receiving positive or negative rewards at each step based on its performance. During this process, the agent tries to learn an optimal decision-making policy, namely which action selections at each state will help to maximize the expected total reward in the long term. This technique is ideal for optimal control problems, games, and many other domains.\n</p>\n\n<p>\n    Many RL architectures use a discrete set of actions to represent a continuous Cartesian action space, and the agent is called to select one of these discrete actions at each time step. Usually, this discretization of a continuous action space reduces the ability of the agent to take actions that perform best, since the agent is forced to choose among the discrete actions.\n</p>\n\n<p>\n    There are two alternative solutions to this problem: either increase the density of discrete points, which affects the responsiveness of the agent, or adopt a discretization of variable resolution which adapts to the needs of the problem. In this thesis, we present a method for creating discretizations able to adapt dynamically according to the use of the action space. The proposed adaptive discretization can match automatically a wide variety of different patterns in a few adaptation steps while maintaining a constant number of discrete points. We embed this adaptive discretization method into the action space of a particular Deep RL agent performing in specific environments that require precision. Our adaptive discretizations take advantage of the selective use the agent makes over the action space and adjust the density of the discrete points in the space, giving an increased number of discrete actions and thus higher resolution to regions where it is needed. As a result, the agentâ€™s precision and learning performance is increased, without a significant increase in computational resources.\n</p>\n\n<h2>References</h2>\n\n<p>My <a href=\"http://purl.tuc.gr/dl/dias/33218A13-C811-425E-BC8B-8D5226842B6F\">Diploma Thesis PDF</a></p>\n\n<p>Based on <a href=\"https://arxiv.org/abs/1512.07679\">this paper</a></p>\n",
    "link": "https://github.com/jimkon/Adaptation-of-Action-Space-for-Reinforcement-Learning",
    "tags": ["All", "Reinforcement Learning", "Neural Networks", "Artificial Intelligence", "Algorithms", "Python"],
    "show": true
  }

]
